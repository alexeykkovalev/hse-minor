{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCF_full",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wdnw3tE9Deb1",
        "colab_type": "text"
      },
      "source": [
        "# Neural Collaborative Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG1IgkJFLSnW",
        "colab_type": "text"
      },
      "source": [
        "## Matrix factorization algorithm\n",
        "\n",
        "NCF - это нейронная модель матричной факторизации, которая объединяет Generalized Matrix Factorization (GMF) и Multi-Layer Perceptron (MLP), объединяя в себе сильные стороны линейности MF и нелинейности MLP для моделирования скрытых структур user-item.\n",
        "\n",
        "Схема архитектуры NCF:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGfys8qiLTs7",
        "colab_type": "text"
      },
      "source": [
        "<img src=\"https://recodatasets.blob.core.windows.net/images/NCF.svg?sanitize=true\">\n",
        "\n",
        "На схеме видно, как используются скрытые вектора пользователей и айтемов и как затем объединяются выходы из слоя GMF (слева) и слоя MLP (справа).\n",
        "\n",
        "### 1.1 Модель GMF\n",
        "\n",
        "В ALS, матрицу оценок можно записать как:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = q _ { i } ^ { T } p _ { u }$$\n",
        "\n",
        "GMF представляет собой слой NCF как стандартный выходной слой MF. Поэтому MF может быть легко обобщена и расширена. Например, если мы позволим веса ребер выходно слоя обучаться без общего ограничения - это даст вариант MF, который позволяет варьировать важность скрытых измерений. А если мы будем использовать нелинейную функцию активации, это даст обобщение MF до нелинейной формы, которая может быть более выразительной чем линейная MF модель. GMF может быть записана как:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = a _ { o u t } \\left( h ^ { T } \\left( q _ { i } \\odot p _ { u } \\right) \\right)$$\n",
        "\n",
        "где $\\odot$ - поэлементное произведение векторов, ${a}_{out}$ и ${h}$ обозначают функцию активации и веса ребер выходного слоя соответственно. MF может рассматриваться как частный случай GMF. Интуитивно, если мы используем тождественную функцию для ${a}_{out}$ и выбираем единичный вектор в качестве ${h}$, то мы в точности повторяем модель MF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHuih2qGLppl",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### 1.2 Модель MLP\n",
        "\n",
        "NCF использует два способа при моделировании рейтингов:\n",
        "\n",
        "1) поэлементное произведение векторов,\n",
        "2) конкатенация векторов.\n",
        "\n",
        "Сразу после контатенации скрытых признаков пользователей и айтемов применяется стандартная модель MLP. Это дает возможность наделить модель большим уровнем гибкости и нелинейности для изучения взаимодействий между $p_{u}$ и $q_{i}$. \n",
        "\n",
        "Запишем модель MLP модель более строго:\n",
        "\n",
        "Для входного слоя, используется конкатенация векторов пользователей и айтемов:\n",
        "\n",
        "$$z _ { 1 } = \\phi _ { 1 } \\left( p _ { u } , q _ { i } \\right) = \\left[ \\begin{array} { c } { p _ { u } } \\\\ { q _ { i } } \\end{array} \\right]$$\n",
        "\n",
        "Для скрытых и выходного слоев MLP запись имеет вид:\n",
        "\n",
        "$$\n",
        "\\phi _ { l } \\left( z _ { l } \\right) = a _ { o u t } \\left( W _ { l } ^ { T } z _ { l } + b _ { l } \\right) , ( l = 2,3 , \\ldots , L - 1 )\n",
        "$$\n",
        "\n",
        "и:\n",
        "\n",
        "$$\n",
        "\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\phi \\left( z _ { L - 1 } \\right) \\right)\n",
        "$$\n",
        "\n",
        "где ${ W }_{ l }$, ${ b }_{ l }$, и ${ a }_{ out }$ обозначают матрицу весов, вектор свободных членов и функцию активации для $l$-ого слоя, соответственно. В качестве функций активации MLP слоев, мы вольны выбирать любую: сигмоиду, гиперболический тангенс, ReLU и другие. В качестве функции активации на выходном слое используется сигмоида $\\sigma(x)=\\frac{1}{1+e^{-x}}$, чтобы ограничить оценки диапазоном (0,1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SmWKJn1Lt7y",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Смешивание GMF и MLP\n",
        "\n",
        "Чтобы обеспечить большую гибкость нашей смешанной модели мы позволяем GMF и MLP обучать независимые эмбединги и затем комбинируем две модели объединяя их последние скрытие слои. Мы взяли $\\phi^{GMF}$ из GMF:\n",
        "\n",
        "$$\\phi _ { u , i } ^ { G M F } = p _ { u } ^ { G M F } \\odot q _ { i } ^ { G M F }$$\n",
        "\n",
        "и получили $\\phi^{MLP}$ из MLP:\n",
        "\n",
        "$$\\phi _ { u , i } ^ { M L P } = a _ { o u t } \\left( W _ { L } ^ { T } \\left( a _ { o u t } \\left( \\ldots a _ { o u t } \\left( W _ { 2 } ^ { T } \\left[ \\begin{array} { c } { p _ { u } ^ { M L P } } \\\\ { q _ { i } ^ { M L P } } \\end{array} \\right] + b _ { 2 } \\right) \\ldots \\right) \\right) + b _ { L }\\right.$$\n",
        "\n",
        "Наконец, мы смешали выходы из GMF и MLP:\n",
        "\n",
        "$$\\hat { r } _ { u , i } = \\sigma \\left( h ^ { T } \\left[ \\begin{array} { l } { \\phi ^ { G M F } } \\\\ { \\phi ^ { M L P } } \\end{array} \\right] \\right)$$\n",
        "\n",
        "Модель сочетает линейность MF и нелинейность DNN при моделировании скрытых user–item структур."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDJwiFk8LwXr",
        "colab_type": "text"
      },
      "source": [
        "### 1.4 Целевая функция\n",
        "\n",
        "Мы можем записать функцию правдоподобия как:\n",
        "\n",
        "$$P \\left( \\mathcal { R } , \\mathcal { R } ^ { - } | \\mathbf { P } , \\mathbf { Q } , \\Theta \\right) = \\prod _ { ( u , i ) \\in \\mathcal { R } } \\hat { r } _ { u , i } \\prod _ { ( u , j ) \\in \\mathcal { R } ^{ - } } \\left( 1 - \\hat { r } _ { u , j } \\right)$$\n",
        "\n",
        "Где $\\mathcal{R}$ обозначает множество наблюдаемых взаимодействий пользователя, а $\\mathcal{ R } ^ { - }$ обозначает множество негативных наблюдений. $\\mathbf{P}$ и $\\mathbf{Q}$ - это скрытая матрица признаков пользователей и айтемов соответственно, $\\Theta$ - параметры модели. Взяв со знаком минус логарифм от правдоподобия мы получим целевую функцию для минимизации NCF алгоритма. Что-то напоминает, не правда ли? https://en.wikipedia.org/wiki/Cross_entropy\n",
        "\n",
        "$$L = - \\sum _ { ( u , i ) \\in \\mathcal { R } \\cup { \\mathcal { R } } ^ { - } } r _ { u , i } \\log \\hat { r } _ { u , i } + \\left( 1 - r _ { u , i } \\right) \\log \\left( 1 - \\hat { r } _ { u , i } \\right)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAyBExMGLWJC",
        "colab_type": "text"
      },
      "source": [
        "## Tensorflow NCF implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YdVsP6LElUF",
        "colab_type": "text"
      },
      "source": [
        "### Downloading dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyKykR2fEqDB",
        "colab_type": "code",
        "outputId": "a63e0b2f-5a34-489f-d392-6e5195223c4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.negative\n",
        "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.rating\n",
        "!wget https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.train.rating"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-06 05:33:57--  https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.negative\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2891424 (2.8M) [text/plain]\n",
            "Saving to: ‘ml-1m.test.negative’\n",
            "\n",
            "ml-1m.test.negative 100%[===================>]   2.76M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2020-02-06 05:34:02 (34.0 MB/s) - ‘ml-1m.test.negative’ saved [2891424/2891424]\n",
            "\n",
            "--2020-02-06 05:34:03--  https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.test.rating\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128039 (125K) [text/plain]\n",
            "Saving to: ‘ml-1m.test.rating’\n",
            "\n",
            "ml-1m.test.rating   100%[===================>] 125.04K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2020-02-06 05:34:03 (4.03 MB/s) - ‘ml-1m.test.rating’ saved [128039/128039]\n",
            "\n",
            "--2020-02-06 05:34:04--  https://raw.githubusercontent.com/hexiangnan/neural_collaborative_filtering/master/Data/ml-1m.train.rating\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20982911 (20M) [text/plain]\n",
            "Saving to: ‘ml-1m.train.rating’\n",
            "\n",
            "ml-1m.train.rating  100%[===================>]  20.01M  47.1MB/s    in 0.4s    \n",
            "\n",
            "2020-02-06 05:34:05 (47.1 MB/s) - ‘ml-1m.train.rating’ saved [20982911/20982911]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3a-EgDuDYUw",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UL1pN8cfG4j",
        "colab_type": "code",
        "outputId": "8e917b06-e95b-4943-910c-baf3069d8e20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import math\n",
        "import heapq\n",
        "import scipy.sparse as sp\n",
        "import multiprocessing\n",
        "from six.moves import xrange\n",
        "\n",
        "from time import time\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "TF version: 2.1.0\n",
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32-nxuu6Db6M",
        "colab_type": "text"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORrCQPT4uZgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ml-1m dataset contains 1,000,209 anonymous ratings of approximately 3,706 movies made by 6,040 users who joined MovieLens in 2000.\n",
        "# All ratings are contained in the file \"ratings.dat\" without header row, and are in the following format:\n",
        "# UserID::MovieID::Rating::Timestamp\n",
        "#\n",
        "# - UserIDs range between 1 and 6040.\n",
        "# - MovieIDs range between 1 and 3952.\n",
        "# - Ratings are made on a 5-star scale (whole-star ratings only).\n",
        "\n",
        "FILE_NAME = 'ml-1m'\n",
        "\n",
        "USER_COLUMN = \"user_id\"\n",
        "ITEM_COLUMN = \"item_id\"  # movies\n",
        "RATING_COLUMN = \"rating\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSNns4ibJCj6",
        "colab_type": "text"
      },
      "source": [
        "### Data loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7hN0q1KZFGw",
        "colab_type": "text"
      },
      "source": [
        "Данные предварительно предобработаны:\n",
        "\n",
        "**train.rating:**\n",
        "- Train file.\n",
        "- Each Line is a training instance: userID itemID rating timestamp (if have)\n",
        "\n",
        "**test.rating:**\n",
        "- Test file (positive instances). \n",
        "- Each Line is a testing instance: userID itemID rating timestamp (if have)\n",
        "\n",
        "**test.negative**\n",
        "- Test file (negative instances).\n",
        "- Each line corresponds to the line of test.rating, containing 99 negative samples.  \n",
        "- Each line is in the format: (userID,itemID) negativeItemID1 negativeItemID2 ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_6RifGb_rI3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Ничего интересного - чтение данных из файлов (можно пропустить).\n",
        "\n",
        "def load_rating_file_as_list(filename):\n",
        "    ratingList = []\n",
        "    with open(filename+'.test.rating', \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            user, item = int(arr[0]), int(arr[1])\n",
        "            ratingList.append([user, item])\n",
        "            line = f.readline()\n",
        "    return ratingList\n",
        "\n",
        "def load_negative_file(filename):\n",
        "    negativeList = []\n",
        "    with open(filename+'.test.negative', \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            negatives = []\n",
        "            for x in arr[1: ]:\n",
        "                negatives.append(int(x))\n",
        "            negativeList.append(negatives)\n",
        "            line = f.readline()\n",
        "    return negativeList\n",
        "\n",
        "def load_rating_file_as_matrix(filename):\n",
        "    '''\n",
        "    Read .rating file and Return dok matrix.\n",
        "    '''\n",
        "    # Get number of users and items\n",
        "    num_users, num_items = 0, 0\n",
        "    with open(filename+'.train.rating', \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            u, i = int(arr[0]), int(arr[1])\n",
        "            num_users = max(num_users, u)\n",
        "            num_items = max(num_items, i)\n",
        "            line = f.readline()\n",
        "    # Construct matrix\n",
        "    mat = sp.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
        "    with open(filename+'.train.rating', \"r\") as f:\n",
        "        line = f.readline()\n",
        "        while line != None and line != \"\":\n",
        "            arr = line.split(\"\\t\")\n",
        "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
        "            if (rating > 0):\n",
        "                mat[user, item] = 1.0\n",
        "            line = f.readline()    \n",
        "    return mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZE5OBXGAdyz",
        "colab_type": "code",
        "outputId": "19973cda-366e-4b87-d007-e764aebe604e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t1 = time()\n",
        "\n",
        "# Loading data\n",
        "train, testRatings, testNegatives = load_rating_file_as_matrix(FILE_NAME), load_rating_file_as_list(FILE_NAME), load_negative_file(FILE_NAME)\n",
        "num_users, num_items = train.shape\n",
        "\n",
        "print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
        "      %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Load data done [14.4 s]. #user=6040, #item=3706, #train=994169, #test=6040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "134bmv7QEDhR",
        "colab_type": "text"
      },
      "source": [
        "### Metrics functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsjzYANX9cJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Расчет метрик: Hit_Ratio, NDCG для top-K рекомендаций\n",
        "\n",
        "# Global variables that are shared across processes\n",
        "_model = None\n",
        "_testRatings = None\n",
        "_testNegatives = None\n",
        "_K = None\n",
        "\n",
        "def getHitRatio(ranklist, gtItem):\n",
        "    for item in ranklist:\n",
        "        if item == gtItem:\n",
        "            return 1\n",
        "    return 0\n",
        "\n",
        "def getNDCG(ranklist, gtItem):\n",
        "    for i in xrange(len(ranklist)):\n",
        "        item = ranklist[i]\n",
        "        if item == gtItem:\n",
        "            return math.log(2) / math.log(i+2)\n",
        "    return 0\n",
        "\n",
        "def eval_one_rating(idx):\n",
        "    rating = _testRatings[idx]\n",
        "    items = _testNegatives[idx]\n",
        "    u = rating[0]\n",
        "    gtItem = rating[1]\n",
        "    items.append(gtItem)\n",
        "    # Get prediction scores\n",
        "    map_item_score = {}\n",
        "    users = np.full(len(items), u, dtype = 'int32')\n",
        "    predictions = _model.predict([users, np.array(items)], \n",
        "                                 batch_size=100, verbose=0)\n",
        "    for i in xrange(len(items)):\n",
        "        item = items[i]\n",
        "        map_item_score[item] = predictions[i]\n",
        "    items.pop()\n",
        "    \n",
        "    # Evaluate top rank list\n",
        "    ranklist = heapq.nlargest(_K, map_item_score, key=map_item_score.get)\n",
        "    hr = getHitRatio(ranklist, gtItem)\n",
        "    ndcg = getNDCG(ranklist, gtItem)\n",
        "    return (hr, ndcg)\n",
        "\n",
        "def evaluate_model(model, testRatings, testNegatives, K, num_thread):\n",
        "\n",
        "    global _model\n",
        "    global _testRatings\n",
        "    global _testNegatives\n",
        "    global _K\n",
        "    _model = model\n",
        "    _testRatings = testRatings\n",
        "    _testNegatives = testNegatives\n",
        "    _K = K\n",
        "        \n",
        "    hits, ndcgs = [],[]\n",
        "    if(num_thread > 1): # Multi-thread\n",
        "        pool = multiprocessing.Pool(processes=num_thread)\n",
        "        res = pool.map(eval_one_rating, range(len(_testRatings)))\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "        hits = [r[0] for r in res]\n",
        "        ndcgs = [r[1] for r in res]\n",
        "        return (hits, ndcgs)\n",
        "    # Single thread\n",
        "    for idx in xrange(len(_testRatings)):\n",
        "        (hr,ndcg) = eval_one_rating(idx)\n",
        "        hits.append(hr)\n",
        "        ndcgs.append(ndcg)      \n",
        "    return (hits, ndcgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQC7ggbEJXSo",
        "colab_type": "text"
      },
      "source": [
        "### Model building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2eCKo2fnfsFs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mf_regularization = 0. #regularization factor for MF embeddings\n",
        "\n",
        "def get_model(num_users, num_items, mf_dim=10, model_layers=[16, 8], mlp_reg_layers=[0., 0.], reg_mf=0):\n",
        "    assert len(model_layers) == len(mlp_reg_layers)\n",
        "\n",
        "    # Input variables\n",
        "\n",
        "    user_input = tf.keras.layers.Input(\n",
        "      shape=(1,), name=USER_COLUMN, dtype=tf.int32)\n",
        "    \n",
        "    item_input = tf.keras.layers.Input(\n",
        "      shape=(1,), name=ITEM_COLUMN, dtype=tf.int32)\n",
        "\n",
        "    # Embedding layer\n",
        "\n",
        "    # Initializer for embedding layers\n",
        "    embedding_initializer = \"glorot_uniform\"\n",
        "\n",
        "    # It turns out to be significantly more effecient to store the MF and MLP\n",
        "    # embedding portions in the same table, and then slice as needed.\n",
        "    embedding_user = tf.keras.layers.Embedding(\n",
        "        num_users,\n",
        "        mf_dim + model_layers[0] // 2,\n",
        "        embeddings_initializer=embedding_initializer,\n",
        "        embeddings_regularizer=tf.keras.regularizers.l2(mf_regularization),\n",
        "        input_length=1,\n",
        "        name=\"embedding_user\")(\n",
        "            user_input)\n",
        "\n",
        "    embedding_item = tf.keras.layers.Embedding(\n",
        "        num_items,\n",
        "        mf_dim + model_layers[0] // 2,\n",
        "        embeddings_initializer=embedding_initializer,\n",
        "        embeddings_regularizer=tf.keras.regularizers.l2(mf_regularization),\n",
        "        input_length=1,\n",
        "        name=\"embedding_item\")(\n",
        "            item_input)\n",
        "\n",
        "    # GMF part\n",
        "\n",
        "    def mf_slice_fn(x):\n",
        "      x = tf.squeeze(x, [1])\n",
        "      return x[:, :mf_dim]\n",
        "    mf_user_latent = tf.keras.layers.Lambda(\n",
        "        mf_slice_fn, name=\"embedding_user_mf\")(embedding_user)\n",
        "    mf_item_latent = tf.keras.layers.Lambda(\n",
        "        mf_slice_fn, name=\"embedding_item_mf\")(embedding_item)\n",
        "    \n",
        "    # Element-wise multiply\n",
        "    mf_vector = tf.keras.layers.multiply([mf_user_latent, mf_item_latent])\n",
        "\n",
        "    # MLP part\n",
        "\n",
        "    def mlp_slice_fn(x):\n",
        "      x = tf.squeeze(x, [1])\n",
        "      return x[:, mf_dim:]\n",
        "    mlp_user_latent = tf.keras.layers.Lambda(\n",
        "        mlp_slice_fn, name=\"embedding_user_mlp\")(embedding_user)\n",
        "    mlp_item_latent = tf.keras.layers.Lambda(\n",
        "        mlp_slice_fn, name=\"embedding_item_mlp\")(embedding_item)\n",
        "\n",
        "    # Concatenation of two latent features\n",
        "    mlp_vector = tf.keras.layers.concatenate([mlp_user_latent, mlp_item_latent])\n",
        "\n",
        "    num_layer = len(model_layers) # Number of layers in the MLP\n",
        "    for layer in xrange(1, num_layer):\n",
        "      model_layer = tf.keras.layers.Dense(\n",
        "          model_layers[layer],\n",
        "          kernel_regularizer=tf.keras.regularizers.l2(mlp_reg_layers[layer]),\n",
        "          activation=\"relu\")\n",
        "      mlp_vector = model_layer(mlp_vector)\n",
        "\n",
        "    # Concatenate GMF and MLP parts\n",
        "    predict_vector = tf.keras.layers.concatenate([mf_vector, mlp_vector])  \n",
        "    \n",
        "    # Final prediction layer\n",
        "    logits = tf.keras.layers.Dense(\n",
        "        1, activation=None, kernel_initializer=\"lecun_uniform\",\n",
        "        name=RATING_COLUMN)(predict_vector)\n",
        "\n",
        "    model = tf.keras.models.Model([user_input, item_input], logits)\n",
        "\n",
        "    # Print model topology.\n",
        "    model.summary()\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaZg7_TH7FT8",
        "colab_type": "code",
        "outputId": "4f9b041f-e62f-45cc-ce3d-e39cdf546494",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "model = get_model(num_users, num_items)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "user_id (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "item_id (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding_user (Embedding)      (None, 1, 18)        108720      user_id[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_item (Embedding)      (None, 1, 18)        66708       item_id[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "embedding_user_mlp (Lambda)     (None, 8)            0           embedding_user[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding_item_mlp (Lambda)     (None, 8)            0           embedding_item[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding_user_mf (Lambda)      (None, 10)           0           embedding_user[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "embedding_item_mf (Lambda)      (None, 10)           0           embedding_item[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 16)           0           embedding_user_mlp[0][0]         \n",
            "                                                                 embedding_item_mlp[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 10)           0           embedding_user_mf[0][0]          \n",
            "                                                                 embedding_item_mf[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 8)            136         concatenate[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 18)           0           multiply[0][0]                   \n",
            "                                                                 dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "rating (Dense)                  (None, 1)            19          concatenate_1[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 175,583\n",
            "Trainable params: 175,583\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blhjtveG7Rhn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.1), loss='binary_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I17ntIP8OdEP",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9SIDjZWA8so",
        "colab_type": "code",
        "outputId": "2460d8d2-897b-428a-85a4-a048db66548a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# обучается ~ 2 мин.\n",
        "start_time = time()\n",
        "\n",
        "# Init performance\n",
        "topK = 10\n",
        "evaluation_threads = 1 #multiprocessing.cpu_count()\n",
        "\n",
        "(hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
        "hr, ndcg = np.array(hits).mean(), np.array(ndcgs).mean()\n",
        "print(\"Init: HR = %.4f, NDCG = %.4f\" % (hr, ndcg))\n",
        "\n",
        "train_time = time() - start_time\n",
        "print(\"Took %.1f seconds for training.\" % (train_time))\n",
        "\n",
        "best_hr, best_ndcg, best_iter = hr, ndcg, -1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Init: HR = 0.1071, NDCG = 0.0499\n",
            "Took 131.8 seconds for training.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVSPsbGNPyND",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_train_instances(train, num_negatives):\n",
        "    user_input, item_input, labels = [],[],[]\n",
        "    num_users = train.shape[0]\n",
        "    for (u, i) in train.keys():\n",
        "        # positive instance\n",
        "        user_input.append(u)\n",
        "        item_input.append(i)\n",
        "        labels.append(1)\n",
        "        # negative instances\n",
        "        for t in xrange(num_negatives):\n",
        "            j = np.random.randint(num_items)\n",
        "            while (u, j) in train:\n",
        "                j = np.random.randint(num_items)\n",
        "            user_input.append(u)\n",
        "            item_input.append(j)\n",
        "            labels.append(0)\n",
        "    return user_input, item_input, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjN_V5CH75P2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs = 1 #20\n",
        "num_negatives = 10\n",
        "batch_size = 256\n",
        "verbose = 1\n",
        "\n",
        "# Training model\n",
        "for epoch in xrange(num_epochs):\n",
        "    t1 = time()\n",
        "    # Generate training instances\n",
        "    user_input, item_input, labels = get_train_instances(train, num_negatives)\n",
        "    \n",
        "    # Training\n",
        "    hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
        "                      np.array(labels), # labels \n",
        "                      batch_size=batch_size, epochs=1, verbose=0, shuffle=True)\n",
        "    t2 = time()\n",
        "    \n",
        "    # Evaluation\n",
        "    if epoch % verbose == 0:\n",
        "        (hits, ndcgs) = evaluate_model(model, testRatings, testNegatives, topK, evaluation_threads)\n",
        "        hr, ndcg, loss = np.array(hits).mean(), np.array(ndcgs).mean(), hist.history['loss'][0]\n",
        "        print('Iteration %d [%.1f s]: HR = %.4f, NDCG = %.4f, loss = %.4f [%.1f s]' \n",
        "              % (epoch,  t2-t1, hr, ndcg, loss, time()-t2))\n",
        "        if hr > best_hr:\n",
        "            best_hr, best_ndcg, best_iter = hr, ndcg, epoch\n",
        "\n",
        "print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMLt_wYeMQ3f",
        "colab_type": "text"
      },
      "source": [
        "## Полезные ссылки:\n",
        "\n",
        "### Пейпер, на который все ссылаются:\n",
        "Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu & Tat-Seng Chua, Neural Collaborative Filtering, 2017 https://arxiv.org/abs/1708.05031\n",
        "\n",
        "### Туториалы и реализации:\n",
        "##### Microsoft:\n",
        "https://github.com/microsoft/recommenders/blob/master/notebooks/02_model/ncf_deep_dive.ipynb\n",
        "\n",
        "##### Tensorflow:\n",
        "https://github.com/tensorflow/models/tree/master/official/recommendation\n",
        "\n",
        "##### Towards Data Science\n",
        "https://towardsdatascience.com/neural-collaborative-filtering-96cef1009401\n",
        "\n",
        "### Лекции:\n",
        "Е.Соколов. Рекомендательные системы. Лекция 1 https://github.com/hse-ds/iad-applied-ds/blob/master/2020/lectures/lecture01-recommender.pdf\n",
        "\n",
        "Е.Соколов. Рекомендательные системы. Лекция 2 https://github.com/hse-ds/iad-applied-ds/blob/master/2020/lectures/lecture02-recommender.pdf\n",
        "\n"
      ]
    }
  ]
}