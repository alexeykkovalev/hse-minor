{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDqfgGd0bL3C"
   },
   "source": [
    "## Майнор ВШЭ\n",
    "## Прикладные задачи анализа данных 2020\n",
    "\n",
    "### Семинар 15: Обучение с подкреплением: Марковский процесс принятия решений, фреймворк Open AI gym, Q-обучение, аппроксимация Q-функции. \n",
    "\n",
    "Обучение с подкреплением (RL - Reinforcement Learning) является направлением машинного обучения и изучает взаимодействие агента, которому необходимо максимизировать долговременный выигрыш в некоторой среде. Агенту не сообщается сведений о правильности действий, как в большинстве задач машинного обучения, вместо этого агент должен определить выгодные действия самостоятельно испробовав их. Испытание действий и отсроченная награда являются основными отличительными признаками RL.\n",
    "\n",
    "<img src=\"rlIntro.png\" caption=\"Взаимодействия агента со средой\" style=\"width: 300px;\" />\n",
    "\n",
    "Основные составляющие модели RL:\n",
    "* $s_t$ -- состояние среды в момент времени $t$,\n",
    "* $a_t$ -- действие, совершаемое агентом в момент времени $t$,\n",
    "* $r_t$ -- вознаграждение, получаемое агентом при совершении действия $a_t$,\n",
    "* $\\pi$ -- стратегия, отвечает за выбор действия в конкретном состоянии.\n",
    "\n",
    "## 1. Марковский процесс принятия решений\n",
    "В простейших моделях RL среда представляется в виде марковского процесса принятия решений (MDP), где функция перехода определяется как $P(s' |s,a)$, что означает вероятность оказаться в состоянии $s'$ при совершении действия $a$ в состоянии $s$. Вознаграждение теперь определяется как $r(s,a,s')$.\n",
    "\n",
    "<img src=\"mdp.png\" caption=\"Марковский процесс принятия решений\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v5d6HqLaqPLx"
   },
   "source": [
    "## 2. Open AI gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFfz5mCmbL3F"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TCVkq2fwb9R_"
   },
   "outputs": [],
   "source": [
    "#устанавливаем библиотеки для визуализации в colab\n",
    "!apt-get -qq -y install  libnvtoolsext1 > /dev/null\n",
    "!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
    "!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
    "!pip -q install gym\n",
    "!pip -q install pyglet\n",
    "!pip -q install pyopengl\n",
    "# !pip -q install pyvirtualdisplay\n",
    "!pip -q install pyvirtualdisplay==0.2.5\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2zSbMzMUc7gl"
   },
   "outputs": [],
   "source": [
    "# запускаем virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1024, 768))\n",
    "display.start()\n",
    "import os\n",
    "os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n",
    "\n",
    "\n",
    "def show_animation(frames):\n",
    "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    animate = lambda i: patch.set_data(frames[i])\n",
    "    ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n",
    "    return ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SNToaC8SbL3K"
   },
   "source": [
    "На семинаре мы будем пользоваться стандартными средами, реализованными в библиотеке OpenAI Gym (https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "faOB1Hf4bL3K",
    "outputId": "64398a29-fa38-4e6d-f566-ffb9bccc383a"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# создаем окружение\n",
    "env = gym.make(\"MountainCar-v0\")\n",
    "env.reset()\n",
    "# рисуем картинку\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8tZM9TZbL3P"
   },
   "source": [
    "Основные методы класса Env:\n",
    "\n",
    "* reset() - инициализация окружения, возвращает первое наблюдение (состояние)\n",
    "* render() - визуализация текущего состояния среды\n",
    "* step($a$) - выполнить в среде действие a и получить кортеж: $<s_{t+1}, r_t, done, info>$, где done флаг заверешния, а info - дополнительная информация.\n",
    "\n",
    "Прежде чем начать взаимодействие с окружением, нужно использовать метод reset():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "ivlciUwGbL3P",
    "outputId": "e454fa7a-fe10-4e63-8bd8-ecd178c8fdef"
   },
   "outputs": [],
   "source": [
    "state0 = env.reset()\n",
    "print(\"изначальное состояние среды:\", state0)\n",
    "# выполняем действие 2 \n",
    "new_state, reward, done, _ = env.step(2)\n",
    "print(\"новое состояние:\", new_state)\n",
    "print(\"вознаграждение\", reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3GqtZB6bL3T"
   },
   "source": [
    "### Среда MountainCar-v0\n",
    "* Состояния: Type: Box(2)\n",
    "\n",
    "\n",
    "\n",
    "Num | Observation  | Min  | Max  \n",
    "----|--------------|------|----   \n",
    "0   | position     | -1.2 | 0.6\n",
    "1   | velocity     | -0.07| 0.07\n",
    "\n",
    "\n",
    "* Действия: Type: Discrete(3)\n",
    "\n",
    "\n",
    "\n",
    "Num | Action|\n",
    "----|-------------|\n",
    "0   | push left   |\n",
    "1   | no push     |\n",
    "2   | push right  |\n",
    "\n",
    "* Вознаграждения: -1 за каждый шаг, пока не достигнута цель \n",
    "\n",
    "* Начальное состояние: Случайная позиция от -0.6 до -0.4 с нулевой скоростью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4R7ScpfFbL3T"
   },
   "source": [
    "### Задание 1\n",
    "В среде MountainCar-v0 мы хотим, чтобы тележка достигла флага. Давайте решим эту задачу, не используя обучение с подкреплением. Модифицируйте код функции act() ниже для выполнения этого задания. Функция получает на вход состояние среды и должна вернуть действие. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C8aUhYCUbL3U"
   },
   "outputs": [],
   "source": [
    "def act(s):\n",
    "    actions = {'left': 0, 'stop': 1, 'right': 2}\n",
    "    \n",
    "    # пример: можем попробовать ехать только влево\n",
    "    # action = actions['left'] \n",
    "    ####### Здесь ваш код ##########\n",
    "    action = actions['left'] if s[1] < 0 else actions[\"right\"]\n",
    "    ################################\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzHsoEhRbL3X"
   },
   "source": [
    "Проверяем решение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "colab_type": "code",
    "id": "HnKJDu5ebL3Y",
    "outputId": "ca1eedcd-3634-428a-bd1c-69577a3c7da5"
   },
   "outputs": [],
   "source": [
    "# создаем окружение, с ограничением на число шагов, используя wrapper (обертку) TimeLimit\n",
    "env = gym.wrappers.TimeLimit(gym.make(\"MountainCar-v0\"), max_episode_steps=250)\n",
    "# проводим инициализацию и запоминаем начальное состояние\n",
    "s = env.reset()\n",
    "done = False\n",
    "\n",
    "frames = []\n",
    "\n",
    "frames.append(env.render(mode = 'rgb_array'))\n",
    "\n",
    "while not done:\n",
    "    # выполняем действие, получаем s, r, done\n",
    "    s, r, done, _ = env.step(act(s))\n",
    "    frames.append(env.render(mode = 'rgb_array'))\n",
    "    # визуализируем окружение\n",
    "    env.render()\n",
    "\n",
    "env.close()\n",
    "if s[0] > 0.47:\n",
    "    print(\"Задание выполнено!\")\n",
    "else:\n",
    "    print(\"\"\"Исправьте функцию выбора действия!\"\"\")\n",
    "\n",
    "HTML(show_animation(frames).to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ajUaHPOJH-3Z"
   },
   "source": [
    "## 3. Q-обучение\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одним из наиболее популярных алгоритм обучения на основе временных различий является Q-обучение. Уравнение Беллмана для значения Q-функции записывается как:\n",
    "\n",
    "$$Q(s,a)=r(s)+\\gamma\\sum_s'T(s,a,s')\\max_{a'}Q(a',s')$$\n",
    "\n",
    "Уравнение для итерационного обновления значений Q-функции выглядит следующим образом:$$Q(s,a)\\leftarrow Q(s,a)+\\alpha(r(s)+\\gamma\\max_{a'}Q(a',s') - Q(s,a)).$$\n",
    "\n",
    "Алгоритм Q-обучения:\n",
    "<img src=\"q.png\">\n",
    "где: s - текущее состояние, a - текущее действие, s', a' - состояние и действие на следующем шаге"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WJ79cSTLLPb-"
   },
   "source": [
    "Для обучения будем использовать среду Taxi-v3. Подробнее про данное окружение можно посмотреть в документации: https://gym.openai.com/envs/Taxi-v3/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151
    },
    "colab_type": "code",
    "id": "ud0_kfOGLUwo",
    "outputId": "64e98d2b-78d5-4381-b59d-260beaaa22db"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Phe3a5aTQBsd"
   },
   "outputs": [],
   "source": [
    "def show_progress(rewards_batch, log, reward_range=None):\n",
    "    \"\"\"\n",
    "    Удобная функция, которая отображает прогресс обучения.\n",
    "    \"\"\"\n",
    "\n",
    "    if reward_range is None:\n",
    "        reward_range = [-990, +10]\n",
    "    mean_reward = np.mean(rewards_batch)\n",
    "    log.append([mean_reward])\n",
    "\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=[8, 4])\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(zip(*log))[0], label='Mean rewards')\n",
    "    plt.legend(loc=4)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_bddYTsDQmVL"
   },
   "outputs": [],
   "source": [
    "# определяем память, в которой будет храниться Q(s,a)\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "show_progress\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# гиперпараметры алгоритма\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "episodes_number = 10001\n",
    "\n",
    "log = []\n",
    "rewards_batch = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pG9YEmftQtu0"
   },
   "source": [
    "### Задание 2\n",
    "\n",
    "Напишите код для формулы Q-обновления, используя известные: alpha, reward, gamma, next_max, old_value (q_table[state, action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "DUthRLiaIuuV",
    "outputId": "aa3b5a76-dad4-40bc-803b-45ebcb308256"
   },
   "outputs": [],
   "source": [
    "for i in range(1, episodes_number):\n",
    "    state = env.reset()\n",
    "\n",
    "    episode, reward, episode_reward = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # выбираем действие, используя eps-greedy исследование среды\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # исследуем среду\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # используем Q-функциthresholdю\n",
    "\n",
    "        # выполняем действие в среде \n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        # получаем old_value (Q(s,a)) и next_max (max(Q(s', a')))\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        # код для Q-обновления\n",
    "        # new_value = \n",
    "        ####### Здесь ваш код ##########\n",
    "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "        ################################\n",
    "        \n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        state = next_state\n",
    "        episode += 1\n",
    "        episode_reward += reward\n",
    "    rewards_batch.append(episode_reward)\n",
    "     \n",
    "    if i % 100 == 0:\n",
    "        show_progress(rewards_batch, log)\n",
    "        rewards_batch = []\n",
    "        print(f\"Episode: {i}, Reward: {episode_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QCvfdLORQOf0"
   },
   "source": [
    "Если все сделано правильно, то график должен выйти на плато около 0. А значение вознаграждение будет в диапазоне [-5, 10], за счет случайного выбора начальной позиции такси и пассажира. Попробуйте изменить гиперпараметры и сравните результаты."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f7sfWBeBq8Wx"
   },
   "source": [
    "## 4. Аппроксимация Q-функции\n",
    "\n",
    "В данном пункте мы будем использовать библиотеку tensorflow для обучения нейронной сети, хотя можно использовать и любую другую библиотеку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BFkc4eN16Lh"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "if 'google.colab' in sys.modules:\n",
    "    %tensorflow_version 1.x\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "colab_type": "code",
    "id": "YRnOxiAZrOFN",
    "outputId": "4eec6d45-f7a1-448b-fcf8-6fd6382149da"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cYbIV7w42Fp1"
   },
   "source": [
    "Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
    "<img src=\"https://github.com/Tviskaron/mipt/raw/a74602b08d18ccc5c65d035b46d6bd323d39384e/RL/04/qlearningscheme.png\">\n",
    "Для начала попробуйте использовать только полносвязные слои (L.Dense) и простые активационные функции. Сигмоиды и другие функции не будут работать с ненормализованными входными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "4kjlhlAP4rcf",
    "outputId": "c19910db-1ad2-4667-d4ae-bb257f010900"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yjFU7TXz4sD7"
   },
   "outputs": [],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "# определяем граф вычислений!\n",
    "####### Здесь ваш код ##########\n",
    "network.add(L.Dense(300, activation=\"relu\"))\n",
    "network.add(L.Dense(n_actions))\n",
    "################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rcZhESiN4zuE"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    сэмплируем (eps greedy) действие  \n",
    "    \"\"\"\n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    # нужно выбрать действия eps-жадно, как мы делали в табличном Q-обучении\n",
    "    # action = \n",
    "    ####### Здесь ваш код ##########\n",
    "    if epsilon < random.random():\n",
    "        action = np.argmax(q_values)\n",
    "    else:\n",
    "        action = random.choice(range(n_actions))\n",
    "    ################################\n",
    "    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "LpYxHgz642cZ",
    "outputId": "57dec09c-b955-4cd5-db13-f91ce59436d3"
   },
   "outputs": [],
   "source": [
    "assert network.output_shape == (None, n_actions), \"Убедитесь, что стратегия переводит s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == keras.activations.linear, \"убедитесь, что вы предсказываете q без нелинейности\"\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \"убедитесь, что возвращается только одно действие (типа integer)\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed'%eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcAUZG5S5LmN"
   },
   "source": [
    "Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2,$$\n",
    "где\n",
    "* $s, a, r, s'$ состояние, действие, вознаграждение и следующее состояние \n",
    "* $\\gamma$ дисконтирующий множетель.\n",
    "\n",
    "Основная тонкость состоит в использовании $Q_{-}(s',a')$. Эта та же самая функция, что и $Q_{\\theta}$, которая является выходом нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. Для этого используется функция tf.stop_gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hpZjPiKP51_5"
   },
   "outputs": [],
   "source": [
    "# Создаем placeholders для <s, a, r, s'>, \n",
    "# не забываем про индикатор окончания эпизода (is_done = True)\n",
    "states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim)\n",
    "actions_ph = keras.backend.placeholder(dtype='int32', shape=[None])\n",
    "rewards_ph = keras.backend.placeholder(dtype='float32', shape=[None])\n",
    "next_states_ph = keras.backend.placeholder(dtype='float32', shape=(None,) + state_dim)\n",
    "is_done_ph = keras.backend.placeholder(dtype='bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0MqsHE1w59un"
   },
   "outputs": [],
   "source": [
    "# получаем q для всех действий, в текущем состоянии\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "# получаем q-values для выбранного действия\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gw_S5wXc6FUs"
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# применяем сеть для получения q-value для next_states_ph\n",
    "# predicted_next_qvalues =\n",
    "####### Здесь ваш код ##########\n",
    "predicted_next_qvalues = network(next_states_ph)\n",
    "################################\n",
    "\n",
    "# вычисляем V*(next_states) \n",
    "# по предсказанным следующим q-values\n",
    "# next_state_values =\n",
    "####### Здесь ваш код ##########\n",
    "next_state_values = tf.reduce_max(predicted_next_qvalues, axis=1)\n",
    "################################\n",
    "\n",
    "# Вычисляем target q-values для функции потерь \n",
    "# target_qvalues_for_actions = \n",
    "####### Здесь ваш код ##########\n",
    "target_qvalues_for_actions = rewards_ph + gamma * next_state_values\n",
    "################################\n",
    "\n",
    "# для последнего значения в эпизоде используем упрощенную формулу Q(s,a) = r(s,a) \n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xO26mQWF61Rd"
   },
   "outputs": [],
   "source": [
    "# mean squared error loss, который будем минимизировать\n",
    "loss = (predicted_qvalues_for_actions - tf.stop_gradient(target_qvalues_for_actions)) ** 2\n",
    "loss = tf.reduce_mean(loss)\n",
    "\n",
    "# применяем AdamOptimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VTImrMM77CTg"
   },
   "outputs": [],
   "source": [
    "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"убедитесь, что обновление выполняется только для выбранного действия\"\n",
    "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"убедитесь, что вы не распространяете градиент Q_(s',a')\"\n",
    "assert predicted_next_qvalues.shape.ndims == 2, \"убедитесь, что вы предсказываете q для всех действий,следующего состояния\"\n",
    "assert next_state_values.shape.ndims == 1, \"убедитесь, что вы вычислили V(s') как максимум только по оси действий, а не по всем осям\"\n",
    "assert target_qvalues_for_actions.shape.ndims == 1, \"что-то не так с целевыми q-значениями, они должны быть вектором\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fCxjK8hM7NQs"
   },
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EqcQCsXb7lAH"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, t_max=1000, epsilon=0, train=False):\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "                next_states_ph: [next_s], is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E1BN4lbq7pe4"
   },
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "5XzMJKbP7s7g",
    "outputId": "7ca3671f-f0db-4af9-f310-4e8ed8a865c2"
   },
   "outputs": [],
   "source": [
    "for i in range(1, 1000):\n",
    "    session_rewards = [generate_session(env, epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    if i % 10 == 0:\n",
    "          clear_output(wait=True)\n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"убедитесь, что epsilon не становится < 0\"\n",
    "    \n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print(\"Принято!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM3vch0F9LuN"
   },
   "source": [
    "Интерпретация результатов\n",
    "* mean reward - среднее вознаграждение за эпизод. В случае корректной реализации, этот показатель будет низким первые 5 эпох и только затем будет возрастать и сойдется на 20-30 эпохе в зависимости от архитектуры сети. \n",
    "* Если сеть не достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте $\\epsilon$. \n",
    "* epsilon -- обеспечивает стремление агента исследовать среду. Можно искусственно изменять малые значения $\\epsilon$ при низких результатах на 0.1 - 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7FVcOWucSMrU"
   },
   "source": [
    "Полезные ссылки:\n",
    "* https://github.com/yandexdataschool/Practical_RL\n",
    "* https://www.youtube.com/playlist?list=PLbWDNovNB5mqFBgq7i3MY6Ui4zudcvNFJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sem15.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
